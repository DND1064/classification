# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1haNW7Xw45KWWQYuiuJKiKYsiUvzP47t1
"""

# Install required libraries
!pip install -q mlxtend scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler
from mlxtend.plotting import plot_decision_regions

# Load the iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='target')
target_names = iris.target_names
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target

# Describe the data
print("Data Description:")
print(X.describe())
print("\nInfo:")
print(X.info())
print("\nFirst 5 rows:")
print(X.head())

# Scale features for MultinomialNB (requires non-negative features)
scaler = MinMaxScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# --- Naive Bayes Models ---

# Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred_gnb = gnb.predict(X_test)
accuracy_gnb = accuracy_score(y_test, y_pred_gnb)
print(f"\nGaussian Naive Bayes Accuracy: {accuracy_gnb:.4f}")

# Multinomial Naive Bayes
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_pred_mnb = mnb.predict(X_test)
accuracy_mnb = accuracy_score(y_test, y_pred_mnb)
print(f"Multinomial Naive Bayes Accuracy: {accuracy_mnb:.4f}")

# --- Plotting Decision Regions (only first 2 features) ---
X_train_plot = X_train.iloc[:, :2].values
y_train_plot = y_train.values

# Gaussian NB Plot
fig, ax = plt.subplots()
plot_decision_regions(X_train_plot, y_train_plot, clf=GaussianNB().fit(X_train_plot, y_train_plot), legend=2)
plt.xlabel('sepal length (scaled)')
plt.ylabel('sepal width (scaled)')
plt.title('Gaussian Naive Bayes Decision Regions')
plt.show()

# Multinomial NB Plot
fig, ax = plt.subplots()
plot_decision_regions(X_train_plot, y_train_plot, clf=MultinomialNB().fit(X_train_plot, y_train_plot), legend=2)
plt.xlabel('sepal length (scaled)')
plt.ylabel('sepal width (scaled)')
plt.title('Multinomial Naive Bayes Decision Regions')
plt.show()

print(iris.data,iris.target)
target = iris.target
target_names = iris.target_names
df['species_name'] = df['species'].apply(lambda x: iris.target_names[x])
print(df[['species', 'species_name']])

print(df.head())  ## 0-> sentosa, 1-> versicolor, 2-> virginica

# prompt: use some random dataset which has input and output datasets in binary form to perform Bernoulli NB and use mlxtend,plotting for plotting the curve  and describe the dataset and use pd.dataframe to show the dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from mlxtend.plotting import plot_decision_regions

# Install required libraries
!pip install -q mlxtend scikit-learn

# Generate a synthetic binary dataset
# This creates a dataset with 1000 samples, 2 features, 2 classes
# and sets random_state for reproducibility
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, flip_y=0.01,
                           class_sep=1.0, random_state=42)

X_binary = (X > 0).astype(int)
# Convert to pandas DataFrame for easier handling and description
X_df = pd.DataFrame(X_binary, columns=['feature1', 'feature2'])
y_series = pd.Series(y, name='target')

# Describe the dataset
print("Data Description:")
print(X_df.describe())
print("\nInfo:")
print(X_df.info())
print("\nFirst 5 rows of features:")
print(X_df.head())
print("\nFirst 5 rows of target:")
print(y_series.head())

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.2, random_state=42)

# Bernoulli Naive Bayes (requires binary features or features representing counts)
# Since our synthetic data is continuous, we need to binarize it for BernoulliNB
# A common way is to check if a feature is greater than some threshold (e.g., 0 or mean)
# Here, we'll use the default binarize=0.0 threshold which effectively treats non-zero values as 1
bnb = BernoulliNB()
bnb.fit(X_train, y_train)
y_pred_bnb = bnb.predict(X_test)
accuracy_bnb = accuracy_score(y_test, y_pred_bnb)
print(f"\nBernoulli Naive Bayes Accuracy: {accuracy_bnb:.4f}")

# Plotting Decision Regions
fig, ax = plt.subplots()
# We train a new classifier instance for plotting on the training data
# and use the original X_train and y_train which are numpy arrays
plot_decision_regions(X_train, y_train, clf=BernoulliNB().fit(X_train, y_train), legend=2)
plt.xlabel('feature1')
plt.ylabel('feature2')
plt.title('Bernoulli Naive Bayes Decision Regions')
plt.show()

